{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599650858373",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Get the wine dataset\n",
    "\n",
    "*Describe data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total rows:  177\n[  2 143 175  49 103  63   9  28  74   8 100  49  95  28  34  79  57 169\n  63 121  56 137  53  52  13  88   0  69 148  27 118  71 108 149  24  97\n  27  86  88   4  37 157 143  18  44  70   6 140  17 105  20  67 170  64\n  86  85  54 103  15 138  56 174  41 126 138 133 168 168  74  77 142 167\n 171  35 115  14  98  70 108  41  80 118  93 141 141 115 160   4   2 138\n   8  68 117 157 118 116  17  26  44 139  50 137  89 149 118   3  36  56\n 139  57   8   2 135 157 111 155  50 150  65  82  24 151 122  28 108 173\n 119  37  35 106  77  85 154  11 101  32  11  69  55 119  92]\nTraining set rows:  141\nTest set rows:  82\n"
    }
   ],
   "source": [
    "# Split data into training and test set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('wine\\wine.csv')\n",
    "print(\"Total rows: \", len(df))\n",
    "# print(df)\n",
    "\n",
    "# wine = np.array(df)\n",
    "# print(wine)\n",
    "\n",
    "train_idxs = np.random.choice(range(len(df)), size=int(0.8*len(df)))\n",
    "print(train_idxs)\n",
    "train = df.iloc[train_idxs]\n",
    "print(\"Training set rows: \",len(train))\n",
    "\n",
    "test_idxs = np.full(len(df), True)\n",
    "test_idxs[train_idxs] = False\n",
    "test = df.iloc[test_idxs]\n",
    "print(\"Test set rows: \",len(test))\n",
    "\n",
    "train.to_csv('wine-train.csv')\n",
    "test.to_csv('wine-test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split training data into training and cross validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_csv('wine-train.csv')\n",
    "train = np.array(df_train)\n",
    "\n",
    "train, valid = train_test_split(train, shuffle=True)\n",
    "\n",
    "# Split test and validation data into X and Y (inputs and labels)\n",
    "train_y, train_X, valid_y, valid_X = train[:, 1], train[:, 2 : ], valid[:, 1], valid[:, 2 : ] # The labels are in column number 2, the Xs are column 3 onwards\n",
    "\n",
    "# Split test data into X and Y (inputs and labels)\n",
    "test = np.array(pd.read_csv('wine-test.csv'))\n",
    "test_y, test_X = test[:, 1], test[:, 2 : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.08333333333333333\n0.0\n1.0\n"
    }
   ],
   "source": [
    "# run a classification tree on the dataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(test_X, test_y)\n",
    "\n",
    "# Evaluate performance on cross validation set\n",
    "pred_y = tree.predict(valid_X)\n",
    "print(mean_squared_error(valid_y, pred_y))\n",
    "# print(tree.get_depth(), tree.get_n_leaves())\n",
    "\n",
    "# Evaluate performance by comparing with test data\n",
    "pred_y = tree.predict(test_X)\n",
    "print(mean_squared_error(test_y, pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Ensembling to improve performance\n",
    "\n",
    "Ensemble the classification tree buy using random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9904761904761905\n0.012195121951219513\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomForest = RandomForestClassifier(n_estimators=80, max_depth=2, max_samples=10) # init random forest\n",
    "randomForest.fit(train_X, train_y) # fit random forest of decision trees\n",
    "# visualise_predictions(randomForest.predict, X, Y) # visualise\n",
    "\n",
    "# Evaluate the ensemble's performance\n",
    "\n",
    "score = randomForest.score(train_X, train_y) # use the model's score method to compute it's accuracy\n",
    "print(score)\n",
    "\n",
    "pred_y = randomForest.predict(test_X) # Using mean squared error with test data\n",
    "print(mean_squared_error(test_y, pred_y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Finding the best models and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Visualising results and summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6: \"A stakeholder asks you which features most affect the response variable (output). Describe how you would organise a test to determine this.\"\n",
    "\n",
    "I would test this by manipulating the input data for the models, such that all the Xs for one feature are set to zero, and then repeating this until every feature has had a chance to be set to zero. I would then compare which result ends up with the biggest difference from the original result which had all features included. This feature when set to zero that correspends to the biggest difference would therefore be the feature that has the greatest influence over the response variable. "
   ]
  }
 ]
}