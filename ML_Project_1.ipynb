{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599682722814",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Get the wine dataset\n",
    "\n",
    "*Describe data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total rows:  177\n[ 48 117 106 114  50  61 117 101 155  87 102 127  85  29  71 130 171  96\n 121 148  31 125 113  76 133 113  31  93 134  31 166  75  28 118 151  53\n  70 139  37 131 116 102 143  14  14 124 176  46 118  44  98  26   5 165\n  32  49 155  90 108 112  66 154   8   3  71  86  14  31 169 147 114 144\n 166 134 122  21  94  75  53 165 150 144  50 113  42 158   6  23  23   6\n  53  80  78 104 173  64  41  54 161  48 142 123  77 106 100 107  17  18\n  34 145 107  57  66 121   0 133  97 126  79  88 164 104  26 149   0 133\n 114 139  95  87 158 126 155   1   4 153  60 147   9 122  13]\nTraining set rows:  141\nTest set rows:  77\n"
    }
   ],
   "source": [
    "# Split data into training and test set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('wine\\wine.csv')\n",
    "print(\"Total rows: \", len(df))\n",
    "# print(df)\n",
    "\n",
    "# wine = np.array(df)\n",
    "# print(wine)\n",
    "\n",
    "train_idxs = np.random.choice(range(len(df)), size=int(0.8*len(df)))\n",
    "print(train_idxs)\n",
    "train = df.iloc[train_idxs]\n",
    "print(\"Training set rows: \",len(train))\n",
    "\n",
    "test_idxs = np.full(len(df), True)\n",
    "test_idxs[train_idxs] = False\n",
    "test = df.iloc[test_idxs]\n",
    "print(\"Test set rows: \",len(test))\n",
    "\n",
    "train.to_csv('wine-train.csv')\n",
    "test.to_csv('wine-test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[4.800e+01 1.000e+00 1.394e+01 ... 1.120e+00 3.100e+00 1.260e+03]\n [1.170e+02 2.000e+00 1.277e+01 ... 7.000e-01 2.120e+00 3.720e+02]\n [1.060e+02 2.000e+00 1.272e+01 ... 8.800e-01 2.420e+00 4.880e+02]\n ...\n [9.000e+00 1.000e+00 1.410e+01 ... 1.250e+00 3.170e+00 1.510e+03]\n [1.220e+02 2.000e+00 1.305e+01 ... 7.300e-01 3.100e+00 3.800e+02]\n [1.300e+01 1.000e+00 1.438e+01 ... 1.200e+00 3.000e+00 1.547e+03]]\nCounter({2.0: 28, 1.0: 27, 3.0: 22})\nCounter({2.0: 43, 1.0: 32, 3.0: 30})\n"
    }
   ],
   "source": [
    "# Split training data into training and cross validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "df_train = pd.read_csv('wine-train.csv')\n",
    "train = np.array(df_train, dtype=float)\n",
    "print(train)\n",
    "\n",
    "train, valid = train_test_split(train, shuffle=True)\n",
    "\n",
    "# Split test and validation data into X and Y (inputs and labels)\n",
    "train_y, train_X, valid_y, valid_X = train[:, 1], train[:, 2 : ], valid[:, 1], valid[:, 2 : ] # The labels are in column number 2, the Xs are column 3 onwards\n",
    "\n",
    "# Split test data into X and Y (inputs and labels)\n",
    "test = np.array(pd.read_csv('wine-test.csv'))\n",
    "test_y, test_X = test[:, 1], test[:, 2 : ]\n",
    "\n",
    "# Look at the balance of classes to make sure that evaluating models on just their accuracy is okay\n",
    "print(Counter(test_y))\n",
    "print(Counter(train_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Fit models to the wine dataset and test performance\n",
    "\n",
    "Using a classification tree on the model. Evaluate the model's performance by comparing its predicted labels with the test labels using accuracy (the fraction of the predictions that were correct).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.9722222222222222\n1.0\n"
    }
   ],
   "source": [
    "# run a classification tree on the dataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(test_X, test_y)\n",
    "\n",
    "# Evaluate performance on cross validation set\n",
    "pred_y = tree.predict(valid_X)\n",
    "print(accuracy_score(valid_y, pred_y))\n",
    "\n",
    "# Evaluate performance by comparing with test data\n",
    "pred_y = tree.predict(test_X)\n",
    "print(accuracy_score(test_y, pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Ensembling to improve performance\n",
    "\n",
    "Ensemble the classification tree model used above buy using random forests. Evaluate model by looking at its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.922077922077922\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomForest = RandomForestClassifier(n_estimators=100, max_depth=2, max_samples=10)\n",
    "randomForest.fit(train_X, train_y) # fit random forest of decision trees\n",
    "\n",
    "# Evaluate the ensemble's performance\n",
    "score = randomForest.score(test_X, test_y) # use the model's score method to compute it's accuracy\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Finding the best models and hyperparameters\n",
    "\n",
    "We have used the following models for supervised learning classification problems so far: Logistic Regression, RandomForests, Support Vector Machines, and K nearest-neighbours. Using sklearn's VotingClassifier, we can ensemble different models and using sklearn's accuracy_score, we can compare the accuracies to find the best single model, or combination of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The best parameters have been found for RandomForestClassifier with an accuracy of 1.0 on the cross validation data, with parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': 3, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} \n\nThe best parameters have been found for LogisticRegression with an accuracy of 0.9166666666666666 on the cross validation data, with parameters: {'C': 0.9974236128941865, 'class_weight': None, 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': None, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False} \n\nThe best parameters have been found for SVC with an accuracy of 0.8055555555555556 on the cross validation data, with parameters: {'C': 2.3958618893715156, 'break_ties': False, 'cache_size': 200, 'class_weight': None, 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.010253382694224846, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': None, 'shrinking': True, 'tol': 0.001, 'verbose': False} \n\nThe best parameters have been found for KNeighborsClassifier with an accuracy of 0.8611111111111112 on the cross validation data, with parameters: {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 6, 'p': 2, 'weights': 'uniform'} \n\nThe best parameters have been found for VotingClassifier with an accuracy of 0.9722222222222222 on the cross validation data, with parameters: {'estimators': (('rf', RandomForestClassifier(max_depth=5, max_samples=3)), ('lr', LogisticRegression(C=0.9974236128941865))), 'flatten_transform': True, 'n_jobs': None, 'verbose': False, 'voting': 'hard', 'weights': None, 'rf': RandomForestClassifier(max_depth=5, max_samples=3), 'lr': LogisticRegression(C=0.9974236128941865), 'rf__bootstrap': True, 'rf__ccp_alpha': 0.0, 'rf__class_weight': None, 'rf__criterion': 'gini', 'rf__max_depth': 5, 'rf__max_features': 'auto', 'rf__max_leaf_nodes': None, 'rf__max_samples': 3, 'rf__min_impurity_decrease': 0.0, 'rf__min_impurity_split': None, 'rf__min_samples_leaf': 1, 'rf__min_samples_split': 2, 'rf__min_weight_fraction_leaf': 0.0, 'rf__n_estimators': 100, 'rf__n_jobs': None, 'rf__oob_score': False, 'rf__random_state': None, 'rf__verbose': 0, 'rf__warm_start': False, 'lr__C': 0.9974236128941865, 'lr__class_weight': None, 'lr__dual': False, 'lr__fit_intercept': True, 'lr__intercept_scaling': 1, 'lr__l1_ratio': None, 'lr__max_iter': 100, 'lr__multi_class': 'auto', 'lr__n_jobs': None, 'lr__penalty': 'l2', 'lr__random_state': None, 'lr__solver': 'lbfgs', 'lr__tol': 0.0001, 'lr__verbose': 0, 'lr__warm_start': False} \n\nOn the training data, model RandomForestClassifier has an accuracy of: 0.9090909090909091\nOn the training data, model LogisticRegression has an accuracy of: 0.8701298701298701\nOn the training data, model SVC has an accuracy of: 0.6623376623376623\nOn the training data, model KNeighborsClassifier has an accuracy of: 0.6883116883116883\nOn the training data, model VotingClassifier has an accuracy of: 0.8441558441558441\n\nTherefore, the best model was RandomForestClassifier with an accuracy of 0.9090909090909091\n\nThis model has the following parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': 3, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import random\n",
    "import itertools\n",
    "\n",
    "\n",
    "def update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model):\n",
    "    # A function to see if this current model gives a better accuracy than any previous models, and then to update the new best model\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(test_X)\n",
    "    accuracy = accuracy_score(test_y, pred_y) # If this accuracy is highest so far, then update the best model with this model\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_model = model\n",
    "    return best_accuracy, best_model\n",
    "\n",
    "def parameter_search(model_name, train_X, train_y, test_X, test_y,  estimators=[], n_samples=100,):\n",
    "            \"\"\"\n",
    "            Hyperparameter search function.\n",
    "\n",
    "            Finds the best parameters of a given model based on whichever parameters give the highest accuracy score.\n",
    "            Works with the following scikit-learn models: RandomForestClassifier, LogisticRegression, SVC and KNeighborsClassifier.\n",
    "\n",
    "            Parameters \n",
    "\n",
    "            model_name:{\"RandomForestClassifier\", \"LogisticRegression\", \"SVC\", \"KNeighborsClassifier\"}\n",
    "                Depending on the model chosen, different hyperparameters will be returned. \n",
    "                For \"RandomForestClassifier\", the n_estimators, max_depth and max_samples will be returned\n",
    "                For \"LogisticRegression\", the C will be returned\n",
    "                For \"SVC\", the C and gamma will be returned\n",
    "                For \"KNeighborsClassifier\", the n_neighbours will be returned.\n",
    "\n",
    "            train_X, train_y, test_X, test_y: numpy array\n",
    "                This will take in the training data and test data in the order of the training Xs then ys, then the test Xs then ys.\n",
    "\n",
    "            n_samples: int, default=1000\n",
    "                How many samples of each hyperparameter are required, meaning a total of (n_samples**number of hyperparameters) samples will be taken.\n",
    "\n",
    "            \"\"\"\n",
    "            # Initialise best accuracy and best model variables\n",
    "            best_accuracy = 0\n",
    "            best_model = None\n",
    "\n",
    "            # Search for different parameteres depending on the model\n",
    "            if model_name == \"RandomForestClassifier\":\n",
    "                for i in range(1, min(len(train_X), n_samples) + 1): # There can't be more samples than there are examples in data\n",
    "                    for j in range(1, n_samples + 1): \n",
    "                        model = RandomForestClassifier(max_samples=i,max_depth=j)\n",
    "                        best_accuracy, best_model = update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model)\n",
    "            elif model_name in [\"LogisticRegression\",\"SVC\"]:\n",
    "                half = int(n_samples/2)\n",
    "                C = random.exponential(scale=1, size=(half)) # let half of the values for C be small (roughly between 0 and 1)\n",
    "                C = np.append(C, 10*random.exponential(scale=100, size=(n_samples-half))) # let other half be big (roughly between 10 and 10000)\n",
    "                for c in C:\n",
    "                    if model_name == \"LogisticRegression\":\n",
    "                        model = LogisticRegression(C=c)\n",
    "                        best_accuracy, best_model = update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model)\n",
    "                    else:\n",
    "                        for g in C:\n",
    "                            model = SVC(C=c, gamma=g)\n",
    "                            best_accuracy, best_model = update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model)\n",
    "            elif model_name == \"KNeighborsClassifier\":\n",
    "                for i in range(1, min(len(train_X), n_samples) + 1): # There can't be more neighbours than there are examples in the data\n",
    "                    # Initialise and test the model on these parameters\n",
    "                    model = KNeighborsClassifier(n_neighbors=i)\n",
    "                    best_accuracy, best_model = update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model)\n",
    "            elif model_name == \"VotingClassifier\":\n",
    "                total_combs_of_estimators = []\n",
    "                for i in range(2, len(estimators) + 1): # get all the different combinations of the models for ensembling\n",
    "                    total_combs_of_estimators.extend(list(itertools.combinations(estimators, i)))\n",
    "                for estimator in total_combs_of_estimators:\n",
    "                    model = VotingClassifier(estimators=estimator, voting='hard')\n",
    "                    best_accuracy, best_model = update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model)\n",
    "            else:\n",
    "                print(\"Please check the documentation and specify a relevant model\")\n",
    "                return\n",
    "            print(\"The best parameters have been found for\", best_model.__class__.__name__, \"with an accuracy of\", best_accuracy, \"on the cross validation data, with parameters:\",best_model.get_params(),\"\\n\")\n",
    "            return best_model\n",
    "\n",
    "\n",
    "\n",
    "def BestModelAndParameter(train_X, train_y, valid_X, valid_y, test_X, test_y):\n",
    "    \n",
    "    # initialise every model with their best parameters, using VALIDATION data not test data\n",
    "    ran_for = parameter_search(\"RandomForestClassifier\",train_X, train_y, valid_X, valid_y, n_samples=10)\n",
    "    log_reg = parameter_search(\"LogisticRegression\",train_X, train_y, valid_X, valid_y)\n",
    "    sup_vec = parameter_search(\"SVC\",train_X, train_y, valid_X, valid_y)\n",
    "    K_near = parameter_search(\"KNeighborsClassifier\",train_X, train_y, valid_X, valid_y)\n",
    "    \n",
    "    estimators=[('rf', ran_for), ('lr', log_reg), ('svc', sup_vec), ('Knear', K_near)]\n",
    "    voting = parameter_search(\"VotingClassifier\",train_X, train_y, valid_X, valid_y,estimators=estimators)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model = None\n",
    "    for model in (ran_for, log_reg, sup_vec, K_near, voting):\n",
    "        best_accuracy, best_model = update_best_model(model, train_X, train_y, test_X, test_y, best_accuracy, best_model)\n",
    "        print(\"On the training data, model\", model.__class__.__name__, \"has an accuracy of:\", accuracy_score(test_y, model.predict(test_X)))\n",
    "    print(\"\\nTherefore, the best model was\", best_model.__class__.__name__, \"with an accuracy of\", best_accuracy)\n",
    "    print(\"\\nThis model has the following parameters:\", best_model.get_params())\n",
    "    return best_model\n",
    "\n",
    "model = BestModelAndParameter(train_X, train_y, valid_X, valid_y, test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Visualising results and summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-4dc34b1be3db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_classification_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualise_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mshow_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\gobli\\Documents\\AI CORE\\utils.py\u001b[0m in \u001b[0;36mshow_data\u001b[1;34m(X, Y, predictions)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import get_classification_data, show_data, visualise_predictions, colors\n",
    "\n",
    "show_data(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6: \"A stakeholder asks you which features most affect the response variable (output). Describe how you would organise a test to determine this.\"\n",
    "\n",
    "I would test this by manipulating the input data for the models, such that all the Xs for one feature are set to zero, and then repeating this until every feature has had a chance to be set to zero. I would then compare which result ends up with the biggest difference from the original result which had all features included. This feature when set to zero that correspends to the biggest difference would therefore be the feature that has the greatest influence over the response variable. "
   ]
  }
 ]
}