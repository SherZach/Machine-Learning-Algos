{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599682722814",
   "display_name": "Python 3.8.5 32-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Get the wine dataset\n",
    "\n",
    "*Describe data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total rows:  177\n[ 23  14  95  26  11   0  31  81  49  93   2  33  96  13 124 173  49  73\n  39 119  32 125 149  66 148  64 113  73  64  12  41  67 143 176  45   2\n 155 171  61 103  96  12  76 139 172 103 109   5 155 109 107 133   4  21\n   6 114  20  58 154  62 149 123 130  80  72  10 175  37  77 102 150 140\n 128 109 163 154 104 139 153  28  43 124 138  57 159 120 119  92 170  55\n 106  73  60  57 100  63  32  74 118 159  11  69   2  80 165 156  56  22\n 107  86 173 141  32 115 158   5 140 100  17 171 128 132  61 122 176  92\n  76 134 163 101 102  72 170 114  14  42  75 133  85 160   9]\nTraining set rows:  141\nTest set rows:  77\n"
    }
   ],
   "source": [
    "# Split data into training and test set\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('wine\\wine.csv')\n",
    "print(\"Total rows: \", len(df))\n",
    "# print(df)\n",
    "\n",
    "# wine = np.array(df)\n",
    "# print(wine)\n",
    "\n",
    "train_idxs = np.random.choice(range(len(df)), size=int(0.8*len(df)))\n",
    "print(train_idxs)\n",
    "train = df.iloc[train_idxs]\n",
    "print(\"Training set rows: \",len(train))\n",
    "\n",
    "test_idxs = np.full(len(df), True)\n",
    "test_idxs[train_idxs] = False\n",
    "test = df.iloc[test_idxs]\n",
    "print(\"Test set rows: \",len(test))\n",
    "\n",
    "train.to_csv('wine-train.csv')\n",
    "test.to_csv('wine-test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split training data into training and cross validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_csv('wine-train.csv')\n",
    "train = np.array(df_train)\n",
    "\n",
    "train, valid = train_test_split(train, shuffle=True)\n",
    "\n",
    "# Split test and validation data into X and Y (inputs and labels)\n",
    "train_y, train_X, valid_y, valid_X = train[:, 1], train[:, 2 : ], valid[:, 1], valid[:, 2 : ] # The labels are in column number 2, the Xs are column 3 onwards\n",
    "\n",
    "# Split test data into X and Y (inputs and labels)\n",
    "test = np.array(pd.read_csv('wine-test.csv'))\n",
    "test_y, test_X = test[:, 1], test[:, 2 : ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Fit models to the wine dataset and test performance\n",
    "\n",
    "Using a classification tree on the model. Evaluate the model's performance by comparing its predicted labels with the test labels using mean squared error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.16666666666666666\n0.0\n"
    }
   ],
   "source": [
    "# run a classification tree on the dataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(test_X, test_y)\n",
    "\n",
    "# Evaluate performance on cross validation set\n",
    "pred_y = tree.predict(valid_X)\n",
    "print(mean_squared_error(valid_y, pred_y))\n",
    "# print(tree.get_depth(), tree.get_n_leaves())\n",
    "\n",
    "# Evaluate performance by comparing with test data\n",
    "pred_y = tree.predict(test_X)\n",
    "print(mean_squared_error(test_y, pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Ensembling to improve performance\n",
    "\n",
    "Ensemble the classification tree model used above buy using random forests. Evaluate model by looking at its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.948051948051948\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "randomForest = RandomForestClassifier(n_estimators=100, max_depth=2, max_samples=10)\n",
    "randomForest.fit(train_X, train_y) # fit random forest of decision trees\n",
    "\n",
    "# Evaluate the ensemble's performance\n",
    "score = randomForest.score(test_X, test_y) # use the model's score method to compute it's accuracy\n",
    "print(score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Finding the best models and hyperparameters\n",
    "\n",
    "We have used the following models for supervised learning classification problems so far: Logistic Regression, RandomForests, Support Vector Machines, and K nearest-neighbours. Using sklearn's VotingClassifier, we can ensemble different models and using sklearn's accuracy_score, we can compare the accuracies to find the best single model, or combination of models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "RandomForestClassifier 0.974025974025974\nLogisticRegression 0.922077922077922\nSVC 0.6493506493506493\nKNeighborsClassifier 0.6363636363636364\n[Voting] ....................... (1 of 4) Processing rf, total=   0.1s\n[Voting] ....................... (2 of 4) Processing lr, total=   0.0s\n[Voting] ...................... (3 of 4) Processing svc, total=   0.0s\n[Voting] .................... (4 of 4) Processing Knear, total=   0.0s\nVotingClassifier 0.8181818181818182\n\nTherefore the best model was RandomForestClassifier with an accuracy of 0.974025974025974\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class BestEnsembleParameter():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __call__(self, train_X, train_y, test_X, test_y):\n",
    "        # initialise every model\n",
    "        ran_for = RandomForestClassifier()\n",
    "        log_reg = LogisticRegression()\n",
    "        sup_vec = SVC()\n",
    "        K_near = KNeighborsClassifier()\n",
    "        voting = VotingClassifier(estimators=[('rf', ran_for), ('lr', log_reg), ('svc', sup_vec), ('Knear', K_near)], voting='hard', verbose=True)\n",
    "        # fit every model\n",
    "        best_accuracy = 0\n",
    "        best_model = ''\n",
    "        for model in (ran_for, log_reg, sup_vec, K_near, voting):\n",
    "            model.fit(train_X, train_y)\n",
    "            pred_y = model.predict(test_X)  # do predictions for every model\n",
    "            accuracy = accuracy_score(test_y, pred_y) # get the accuracy of each model\n",
    "            print(model.__class__.__name__, accuracy) # print each model's accuracy\n",
    "            # update the best accuracy score if it is the highest score so far\n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                best_model = model.__class__.__name__\n",
    "        print(\"\\nTherefore, the best model was\", best_model, \"with an accuracy of\", best_accuracy)\n",
    "\n",
    "        def parameter_search():\n",
    "            # random or grid search over parameters\n",
    "            return\n",
    "finder = BestEnsembleParameter()\n",
    "finder(train_X, train_y, test_X, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5: Visualising results and summarise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 6: \"A stakeholder asks you which features most affect the response variable (output). Describe how you would organise a test to determine this.\"\n",
    "\n",
    "I would test this by manipulating the input data for the models, such that all the Xs for one feature are set to zero, and then repeating this until every feature has had a chance to be set to zero. I would then compare which result ends up with the biggest difference from the original result which had all features included. This feature when set to zero that correspends to the biggest difference would therefore be the feature that has the greatest influence over the response variable. "
   ]
  }
 ]
}